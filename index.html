<!DOCTYPE html>
<html>

<head>

  <meta charset="utf-8">
  <!-- 基础SEO -->
  <title>Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots</title>
  <meta name="description"
    content="Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots">
  <meta name="keywords" content="Humanoid Robots, Occupancy Perception, Multimodal">
  <!-- 移动端适配 -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- 预加载 -->
  <!-- 1. 首屏关键CSS -->
  <link rel="preload" href="/static/css/bulma.min.css" as="style">
  <link rel="preload" href="/static/css/index.css" as="style">

  <!-- 2. 首屏关键JS -->
  <link rel="preload" href="/static/js/index.js" as="script">
  <link rel="preload" href="/static/js/bulma-carousel.min.js" as="script">

  <link rel="preload" href="https://media.githubusercontent.com/media/humanoid-occupancy/humanoid-occupancy.github.io/main/static/videos/shots_video.mp4" 
                      as="video" 
                      type="video/mp4" 
                      crossorigin>

  <!-- <link rel="preload" href="https://media.githubusercontent.com/media/humanoid-occupancy/humanoid-occupancy.github.io/main/static/videos/result1.mp4" as="video" type="video/mp4" crossorigin> -->
  <!-- 5. 首屏Logo图片 -->
  <link rel="preload" href="/static/images/x-humanoid-logo.png" as="image">
  <link rel="preload" href="/static/images/giga-logo.png" as="image">
  <script>
    window.dataLayer = window.dataLayer || [];
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <!-- <link href="./static/css/googlecss.css"
        rel="stylesheet"> -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/robot.png">

  <script src="./static/js/jquerymin351.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .author-block {
        display: none;
    }

    .publication-authors:hover .author-block {
        display: block;
    }
</style>

</head>

<script>
// 1. 视口内图片预加载
document.addEventListener('DOMContentLoaded', () => {
  const lazyImages = [
    '/static/images/head1.png',
    '/static/images/head2.png',
    '/static/images/data_collection.png',
    '/static/images/generation_pipeline.png',
    '/static/images/arch_new.png'
  ];
  
  lazyImages.forEach(src => {
    const img = new Image();
    img.src = src;
    img.loading = 'lazy';
  });
});

// 2. 第二段视频预加载（用户滚动到Results部分时）
const observer = new IntersectionObserver((entries) => {
  entries.forEach(entry => {
    if (entry.isIntersecting) {
      const link = document.createElement('link');
      link.rel = 'preload';
      link.as = 'video';
      link.href = 'https://media.githubusercontent.com/media/humanoid-occupancy/humanoid-occupancy.github.io/main/static/videos/result1.mp4';
      link.crossOrigin = 'anonymous';
      document.head.appendChild(link);
      observer.disconnect();
    }
  });
}, {threshold: 0.1});

observer.observe(document.querySelector('#results'));
</script>

<body>

 <nav class="navbar is-fixed-top" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <!-- 网站Logo/名称 -->
      <a class="navbar-item" href="https://x-humanoid.com">
        <span class="icon-text">
          <span class="icon">
            <i class="fas fa-robot"></i>
          </span>
          <span>X-Humanoid</span>
        </span>
      </a>

      <!-- 移动端菜单按钮 -->
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasic">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div id="navbarBasic" class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="#home">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
          <span>Home</span>
        </a>

        <a class="navbar-item" href="#abstract">
          <span class="icon">
            <i class="fas fa-scroll"></i>
          </span>
          <span>Abstract</span>
        </a>

        <a class="navbar-item" href="#data">
          <span class="icon">
            <i class="fas fa-server"></i>
          </span>
          <span>Data</span>
        </a>

        <!-- 数据分析 -->
        <a class="navbar-item" href="#annotation">
          <span class="icon">
            <i class="fas fa-cubes"></i>
          </span>
          <span>Annotation</span>
        </a>

        <a class="navbar-item" href="#method">
          <span class="icon">
            <i class="fas fa-project-diagram"></i>
          </span>
          <span>Method</span>
        </a>

        <a class="navbar-item" href="#results">
          <span class="icon">
            <i class="fas fa-chart-line"></i>
          </span>
          <span>Results</span>
        </a>

        <!-- 引用 -->
        <a class="navbar-item" href="#bibtex">
          <span class="icon">
            <i class="fas fa-quote-right"></i>
          </span>
          <span>Citation</span>
        </a>
      </div>
    </div>
  </nav>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="video-container has-rounded-corners">
          <video id="teaser" autoplay muted loop playsinline>
            <source src="https://media.githubusercontent.com/media/humanoid-occupancy/humanoid-occupancy.github.io/main/static/videos/shots_video.mp4" type="video/mp4">
          </video>
        </div>
        <h2 class="subtitle has-text-centered mt-5">
          We present a multimodal occupancy perception system tailored for humanoid robots, encompassing the complete hardware and software stack, including sensor configurations, data acquisition, data annotation, and perception networks.
        </h2>
      </div>
    </div>
  </section>

  <section class="hero" id="home">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots</h1>
            
            <div class="is-size-5 publication-authors">
              <!-- 直接显示完整作者列表（不再隐藏） -->
              <div class="author-list">
                Wei Cui<sup>*1</sup>, Haoyu Wang<sup>*2</sup>, Wenkang Qin<sup>2</sup>,
                Yijie Guo<sup>1</sup>, Gang Han<sup>1</sup>, Wen Zhao<sup>1</sup>, 
                Jiahang Cao<sup>1</sup>, Zhang Zhang<sup>1</sup>, Jiaru Zhong<sup>1</sup>,
                Jingkai Sun<sup>1</sup>, Pihai Sun<sup>1</sup>, Shuai Shi<sup>1</sup>,
                Botuo Jiang<sup>1</sup>, Zhichao Liu<sup>2</sup>, Yang Wang<sup>2</sup>,
                Zheng Zhu<sup>2</sup>,<br> Guan Huang<sup>2</sup>, Qiang Zhang<sup>†Δ1</sup>,
                and Jian Tang<sup>1</sup>                
              </div>

              <!-- 作者身份说明 -->
              <div class="is-size-6 publication-authors">
                <span>
                  <sup>*</sup>&nbsp;Equal Contributors,&nbsp;
                  <sup>†</sup>&nbsp;Corresponding Author,&nbsp;
                  <sup>Δ</sup>&nbsp;Project and Technical Leader
                </span>
                <br>
                <!-- 机构信息 -->
                <div class="institution-info">
                  <sup>1</sup>&nbsp;X-Humanoid &nbsp;&nbsp;
                  <sup>2</sup>&nbsp;GigaAI<br>
                </div>
              </div>
              
              <!-- 机构Logo -->
              <div class="columns is-centered">
                <!-- 第一个Logo -->
                <div class="column has-text-centered">
                  <div class="logo-image-box">
                    <img src="/static/images/x-humanoid-logo.png" 
                        alt="X-Humanoid Logo"
                        class="logo-image"
                        style="--logo-height: 110px"
                        loading="lazy">
                  </div>
                </div>

                <!-- 第二个Logo -->
                <div class="column has-text-centered">
                  <div class="logo-image-box">
                    <img src="/static/images/giga-logo.png" 
                        alt="Giga AI Logo"
                        class="logo-image"
                        style="--logo-height: 95px"
                        loading="lazy">
                    <!-- <p class="has-text-grey mt-2">Giga AI</p> -->
                  </div>
                </div>
              </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="mailto:jony.zhang@x-humanoid.com"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-envelope"></i>
                    </span>
                    <span>Email</span>
                  </a>
                </span>
                  <!-- <span class="link-block">
                    <a href=""
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Huggingface</span>
                    </a> -->
                  <!-- <span class="link-block">
                    <a href=""
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Github</span>
                    </a> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" id="abstract">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Humanoid robot technology is advancing rapidly, with manufacturers introducing diverse heterogeneous visual perception modules tailored to specific scenarios. Among various perception paradigms, occupancy-based representation has become widely recognized as particularly suitable for humanoid robots, as it provides both rich semantic and 3D geometric information essential for comprehensive environmental understanding.

              In this work, we present Humanoid Occupancy, a generalized multimodal occupancy perception system that integrates hardware and software components, data acquisition devices, and a dedicated annotation pipeline. Our framework employs advanced multi-modal fusion techniques to generate grid-based occupancy outputs encoding both occupancy status and semantic labels, thereby enabling holistic environmental understanding for downstream tasks such as task planning and navigation. To address the unique challenges of humanoid robots, we overcome issues such as kinematic interference and occlusion, and establish an effective sensor layout strategy. Furthermore, we have developed the first panoramic occupancy dataset specifically for humanoid robots, offering a valuable benchmark and resource for future research and development in this domain. The network architecture incorporates multi-modal feature fusion and temporal information integration to ensure robust perception. Overall, Humanoid Occupancy delivers effective environmental perception for humanoid robots and establishes a technical foundation for standardizing universal visual modules, paving the way for the widespread deployment of humanoid robots in complex real-world scenarios.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="data">
    <div class="container is-max-desktop">
      <!-- 标题部分 -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Sensor Layout and Data Acquisition</h2>
          
          <div class="columns is-centered">
            <!-- 图片1 -->
            <div class="column is-half-tablet is-one-third-desktop">
              <div class="image-container">
                <img src="./static/images/head1.png" 
                    alt="Figure 1 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">Side View</p>
              </div>
            </div>
            
            <!-- 图片2 -->
            <div class="column is-half-tablet is-one-third-desktop">
              <div class="image-container">
                <img src="./static/images/head2.png" 
                    alt="Figure 2 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">Isometric View</p>
              </div>
            </div>

            <div class="column is-half-tablet is-one-third-desktop">
              <div class="image-container">
                <img src="./static/images/data_collection.png" 
                    alt="Figure 2 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">Equipment and Schematic Diagram of Data Acquisition Process</p>
              </div>
            </div>
          </div>
          
          <!-- 描述文本 -->
          <div class="content has-text-justified">
            <p><strong>Sensor Layout.</strong>
                Our sensor consists of 6 cameras and a LiDAR.
                The 6 cameras use standard RGB sensors, arranged in a way that one is arranged in the front and back, and two are arranged on each side.
                The horizontal FOV of the camera is 118 degrees and the vertical FOV is 92 degrees.
                The LiDAR uses a 40-line 360-degree omnidirectional LiDAR with a vertical FOV of 59 degrees.</p>
          </div>

          <div class="content has-text-justified">
            <p><strong>Data Acquisition.</strong> 
              We use a wearable device with the same sensor configuration as humanoid robots to collect data. Human data collectors around 160 cm tall wear it directly on their heads to ensure the sensor height matches that of the final installation on the robot, a neck stabilizer is added to prevent head shaking during collection, and the collectors’ walking speed is limited to no more than 1.2 meters per second with turning angular speed not exceeding 0.4 radians per second.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="annotation">
    <div class="container is-max-desktop">
      <!-- 标题部分 -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Annotation Pipeline</h2>
          
          <div class="columns is-centered">
            <!-- 图片1 -->
            <div class="column is-8-desktop">
              <div class="image-container">
                <img src="./static/images/generation_pipeline.png" 
                    alt="Figure 1 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">The Occupancy Generation Pipeline</p>
              </div>
            </div>
          </div> 
          
          <!-- 描述文本 -->
          <div class="content has-text-justified">
            <p>Collected data is divided into home, industrial, and outdoor scenes, with different point-wise semantic categories defined for annotation in each. Bounding boxes are annotated for dynamic objects (pedestrians, cyclists, vehicles); pedestrians are categorized into special-posture and ordinary-posture, with only the latter annotated with bounding boxes. Besides dynamic object bounding boxes, point-by-point semantic annotation is done on point clouds: dynamic objects in each clip are removed, remaining static points are superimposed onto multi-frame point clouds, and point-level semantic annotation is performed by referring to the point cloud’s projection on images. For obtaining occupancy ground truth after annotation, superimposed static background points are aligned to the ego coordinate system of frame-by-frame point clouds, superimposed dynamic foreground points are spliced into the point cloud based on the frame’s dynamic object poses, and the superimposed and spliced point cloud is directly voxelized to get the final occupancy ground truth without Poisson reconstruction.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="method">
    <div class="container is-max-desktop">
      <!-- 标题部分 -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Multi-Modal Fusion Network</h2>
          
          <div class="columns is-centered">
            <!-- 图片1 -->
            <div class="column">
              <div class="image-container">
                <img src="./static/images/arch_new.png" 
                    alt="Figure 1 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">The Model Architecture</p>
              </div>
            </div>

          </div> 
          
          <!-- 描述文本 -->
          <div class="content has-text-justified">
            <p>Our occupancy perception model accepts multimodal inputs, including a LiDAR point cloud and 6 pinhole camera images.
              We use the Bird's Eye View (BEV) paradigm that has been widely validated and adopted in autonomous driving for feature extraction and feature fusion.
              Since the robotic sensors undergo pitch and roll motions during movement, it is essential to transform the sensor data into a gravity-aligned egocentric reference frame to comply with the BEV assumption.
              Specifically, we extract LiDAR and camera features through two modality-specific feature extraction branches, and then perform multi-modal feature fusion through Transformer Decoder.
              The final occupancy result is predicted on the fused BEV features.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="results">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <!-- 标题和描述 -->
          <h2 class="title is-3">Experiments and Results</h2>
          <p>Experiments are conducted on our collected data, including 180 training clips and 20 validation clips. Metrics used for evaluation are mIoU and rayIoU. The perception range is set as [-10m, 10m] for the X and Y axes, and [-1.5m, 0.9m] for the Z-axis in the ego coordinate system. The output occupancy prediction size is 200×200×24 voxels.</p>
          <br>
          
          <h3 class="title is-4">Benchmark</h3>
          <div class="columns is-centered">
            <!-- 图片1 -->
            <div class="column is-half-tablet is-half-desktop">
              <div class="image-container">
                <img src="./static/images/humanoid_occ_mIOU.png" 
                    alt="Figure 1 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">mIoU Performance</p>
              </div>
            </div>
            
            <!-- 图片2 -->
            <div class="column is-half-tablet is-half-desktop">
              <div class="image-container">
                <img src="./static/images/humanoid_occ_rayIOU.png" 
                    alt="Figure 2 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">rayIoU Performance</p>
              </div>
            </div>
          </div>
          <div class="content has-text-justified">
            <p>We benchmark our method against representative BEV perception models on our multi-modal dataset. All models adopt identical training configurations, including input image resolution, backbone network, feature dimensions, and training strategies. Our model achieves superior metrics while maintaining lightweight architecture with significantly fewer parameters.</p>
          </div>

          <h3 class="title is-4">Visualization Result</h3>
          <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
            <source src="https://media.githubusercontent.com/media/humanoid-occupancy/humanoid-occupancy.github.io/main/static/videos/result1.mp4" type="video/mp4">
          </video>

          <!-- 底部链接 -->
          <br>
          <br>
          <p>
            For more details on data analysis and experiment results, 
            please refer to our <a href="">paper</a>.
          </p>
        </div>
      </div>
    </div>
  </section>



  <section class="section" id="bibtex">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>place holder
      </code></pre>
    </div>
    <br>
  </section>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="">
          <i class="fas fa-file-pdf"></i>
        </a>
        <!-- <a class="icon-link" href="" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a> -->
      </div>
      <!-- <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p>
              The website template was borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
              and <a href="https://eureka-research.github.io/">Eureka</a>.
            </p>
          </div>
        </div>
      </div> -->
    </div>
  </footer>

</body>

</html>
