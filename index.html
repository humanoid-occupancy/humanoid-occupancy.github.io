<!DOCTYPE html>
<html>

<head>

  <meta charset="utf-8">
  <!-- 基础SEO -->
  <title>Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots</title>
  <meta name="description"
    content="Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots">
  <meta name="keywords" content="Humanoid Robots, Occupancy Perception, Multimodal">
  <!-- 移动端适配 -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- 预加载 -->
  <link rel="preload" href="/static/css/bulma.min.css" as="style">
  <link rel="preload" href="/static/js/index.js" as="script">
  <link rel="preload" href="https://media.githubusercontent.com/media/humanoid-occupancy/humanoid-occupancy.github.io/main/static/videos/shots_video.mp4" as="video" type="video/mp4" crossorigin>
  <link rel="preload" href="https://media.githubusercontent.com/media/humanoid-occupancy/humanoid-occupancy.github.io/main/static/videos/result1.mp4" as="video" type="video/mp4" crossorigin>

  <script>
    window.dataLayer = window.dataLayer || [];
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <!-- <link href="./static/css/googlecss.css"
        rel="stylesheet"> -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/robot.png">

  <script src="./static/js/jquerymin351.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .author-block {
        display: none;
    }

    .publication-authors:hover .author-block {
        display: block;
    }
</style>

</head>

<body>

 <nav class="navbar is-fixed-top" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <!-- 网站Logo/名称 -->
      <a class="navbar-item" href="https://x-humanoid.com">
        <span class="icon-text">
          <span class="icon">
            <i class="fas fa-robot"></i>
          </span>
          <span>X-Humanoid</span>
        </span>
      </a>

      <!-- 移动端菜单按钮 -->
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasic">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div id="navbarBasic" class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="#home">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
          <span>Home</span>
        </a>

        <a class="navbar-item" href="#abstract">
          <span class="icon">
            <i class="fas fa-scroll"></i>
          </span>
          <span>Abstract</span>
        </a>

        <a class="navbar-item" href="#data">
          <span class="icon">
            <i class="fas fa-server"></i>
          </span>
          <span>Data</span>
        </a>

        <!-- 数据分析 -->
        <a class="navbar-item" href="#annotation">
          <span class="icon">
            <i class="fas fa-cubes"></i>
          </span>
          <span>Annotation</span>
        </a>

        <a class="navbar-item" href="#method">
          <span class="icon">
            <i class="fas fa-project-diagram"></i>
          </span>
          <span>Method</span>
        </a>

        <a class="navbar-item" href="#results">
          <span class="icon">
            <i class="fas fa-chart-line"></i>
          </span>
          <span>Results</span>
        </a>

        <!-- 引用 -->
        <a class="navbar-item" href="#bibtex">
          <span class="icon">
            <i class="fas fa-quote-right"></i>
          </span>
          <span>Citation</span>
        </a>
      </div>
    </div>
  </nav>

  <section class="hero" id="home">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots</h1>
            
            <div class="is-size-5 publication-authors">
              <!-- 直接显示完整作者列表（不再隐藏） -->
              <div class="author-list">
                <!--xxx<sup>1,∗</sup>, xxxx<sup>2,3,∗</sup>, xxx<sup>2,3,∗</sup>,
                xxx<sup>1,∗,†</sup>, xxx<sup>1,∗,†</sup>, xxx<sup>1</sup>, 
                xxx<sup>2,3</sup>
                <span class="icon">
                  <i class="fas fa-envelope"></i>
                </span>,
                xxx<sup>1</sup>
                <span class="icon">
                  <i class="fas fa-envelope"></i>
                </span>
                 -->
                Humanoid Occupancy Team
              </div>

              <!-- 作者身份说明 -->
              <div class="is-size-6 publication-authors">
                <!-- <span>
                  <sup>*</sup>Co-first Authors, 
                  <sup><i class="fas fa-envelope"></i></sup>Corresponding Authors, 
                  <sup>†</sup>Project Leaders
                </span>
                <br> -->
                
                <!-- 机构信息 -->
                <div class="institution-info">
                  <sup>1</sup>Beijing Innovation Center of Humanoid Robotics<br>
                  <sup>2</sup>Giga AI<br>
                </div>
              </div>
              
              <!-- 机构Logo -->
              <div class="columns is-centered">
                <!-- 第一个Logo -->
                <div class="column has-text-centered">
                  <div class="logo-image-box">
                    <img src="/static/images/x-humanoid-logo.png" 
                        alt="X-Humanoid Logo"
                        class="logo-image"
                        style="--logo-height: 110px"
                        loading="lazy">
                  </div>
                </div>

                <!-- 第二个Logo -->
                <div class="column has-text-centered">
                  <div class="logo-image-box">
                    <img src="/static/images/giga-logo.png" 
                        alt="Giga AI Logo"
                        class="logo-image"
                        style="--logo-height: 95px"
                        loading="lazy">
                    <!-- <p class="has-text-grey mt-2">Giga AI</p> -->
                  </div>
                </div>
              </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <!-- <span class="link-block">
                  <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data</span>
                  </a> -->
                  <!-- <span class="link-block">
                    <a href=""
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Huggingface</span>
                    </a> -->
                  <!-- <span class="link-block">
                    <a href=""
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Github</span>
                    </a> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="video-container has-rounded-corners">
          <video id="teaser" autoplay muted loop playsinline>
            <source src="https://media.githubusercontent.com/media/humanoid-occupancy/humanoid-occupancy.github.io/main/static/videos/shots_video.mp4" type="video/mp4">
          </video>
        </div>
        <h2 class="subtitle has-text-centered mt-5">
          We present a multimodal occupancy perception system tailored for humanoid robots, encompassing the complete hardware and software stack, including sensor configurations, data acquisition, data annotation, and perception networks. This system can effectively enhance the environmental perception capability of humanoid robots, providing fine-grained information for manipulation, locomotion, and navigation.
        </h2>
      </div>
    </div>
  </section>

  <section class="section" id="abstract">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Humanoid robot technology is advancing rapidly, with manufacturers introducing diverse heterogeneous visual perception modules tailored to specific scenarios. Among various perception paradigms, occupancy-based representation has become widely recognized as particularly suitable for humanoid robots, as it provides both rich semantic and 3D geometric information essential for comprehensive environmental understanding.

              In this work, we present Humanoid Occupancy, a generalized multimodal occupancy perception system that integrates hardware and software components, data acquisition devices, and a dedicated annotation pipeline. Our framework employs advanced multi-modal fusion techniques to generate grid-based occupancy outputs encoding both occupancy status and semantic labels, thereby enabling holistic environmental understanding for downstream tasks such as task planning and navigation. To address the unique challenges of humanoid robots, we overcome issues such as kinematic interference and occlusion, and establish an effective sensor layout strategy. Furthermore, we have developed the first panoramic occupancy dataset specifically for humanoid robots, offering a valuable benchmark and resource for future research and development in this domain. The network architecture incorporates multi-modal feature fusion and temporal information integration to ensure robust perception. Overall, Humanoid Occupancy delivers effective environmental perception for humanoid robots and establishes a technical foundation for standardizing universal visual modules, paving the way for the widespread deployment of humanoid robots in complex real-world scenarios.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="data">
    <div class="container is-max-desktop">
      <!-- 标题部分 -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Sensor Layout and Data Acquisition</h2>
          
          <div class="columns is-centered">
            <!-- 图片1 -->
            <div class="column is-half-tablet is-one-third-desktop">
              <div class="image-container">
                <img src="./static/images/head1.png" 
                    alt="Figure 1 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">Side View</p>
              </div>
            </div>
            
            <!-- 图片2 -->
            <div class="column is-half-tablet is-one-third-desktop">
              <div class="image-container">
                <img src="./static/images/head2.png" 
                    alt="Figure 2 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">Isometric View</p>
              </div>
            </div>

            <div class="column is-half-tablet is-one-third-desktop">
              <div class="image-container">
                <img src="./static/images/data_collection.png" 
                    alt="Figure 2 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">Equipment and Schematic Diagram of Data Acquisition Process</p>
              </div>
            </div>
          </div>
          
          <!-- 描述文本 -->
          <div class="content has-text-justified">
            <p><strong>Sensor Layout.</strong>
                Our sensor consists of 6 cameras and a LiDAR.
                The 6 cameras use standard RGB sensors, arranged in a way that one is arranged in the front and back, and two are arranged on each side.
                The horizontal FOV of the camera is 118 degrees and the vertical FOV is 92 degrees.
                The LiDAR uses a 40-line 360-degree omnidirectional LiDAR with a vertical FOV of 59 degrees.</p>
          </div>

          <div class="content has-text-justified">
            <p><strong>Data Acquisition.</strong> we use a wearable device to collect data.
              Our device is constructed using the same sensor configuration as that used on humanoid robots.
              So human data collectors can wear the device directly on their heads to collect data in the environment.
              This collection method greatly reduces our collection cost and difficulty.
              In order to ensure that the distribution of collected data is as close as possible to the distribution of real robot data, we require the height of human data collectors to be around 160 cm.
              This ensures that the height of the sensors worn by human collectors is almost the same as the height of the sensors finally installed on the humanoid robot.
              To prevent the head shaking of human collectors during collection, causing the sensor to lose horizontal stability, we also added a neck stabilizer to the collection equipment.
              We limit the walking speed of the collectors to no more than 1.2 meters per second, and require that the angular speed when turning should not be higher than 0.4 radians per second.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="annotation">
    <div class="container is-max-desktop">
      <!-- 标题部分 -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Annotation Pipeline</h2>
          
          <div class="columns is-centered">
            <!-- 图片1 -->
            <div class="column is-8-desktop">
              <div class="image-container">
                <img src="./static/images/generation_pipeline.png" 
                    alt="Figure 1 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">The Occupancy Generation Pipeline</p>
              </div>
            </div>
            
            <!-- <div class="column is-half-tablet is-one-third-desktop">
              <div class="image-container">
                <img src="./static/images/head2.png" 
                    alt="Figure 2 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">Isometric View</p>
              </div>
            </div>

            <div class="column is-half-tablet is-one-third-desktop">
              <div class="image-container">
                <img src="./static/images/data_collection.png" 
                    alt="Figure 2 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">Equipment and Schematic Diagram of Data Acquisition Process</p>
              </div>
            </div>-->
          </div> 
          
          <!-- 描述文本 -->
          <div class="content has-text-justified">
            <p>We divide the collected data into three categories according to the scenes, namely home scenes, industrial scenes and outdoor scenes.
              In the three different scenes, we define different point-wise semantic categories to be annotated.
              we annotate the bounding boxes of dynamic objects, including pedestrians, cyclists and vehicles.
              Pedestrians are non-rigid targets, and sometimes other object points are included when they are annotated using bounding boxes.
              To solve this problem, we divide pedestrians into special posture pedestrians, that is, pedestrians that cannot be represented by bounding boxes, and ordinary posture pedestrians, that is, pedestrians that can be represented by bounding boxes.
              We only annotate ordinary posture pedestrians with bounding boxes.

              In addition to the bounding boxes of dynamic targets, we also perform point-by-point semantic annotation on the point cloud.
              When performing semantic annotation of point clouds, we remove the dynamic targets of each clips and then superimpose the remaining static points on multiple frames of point clouds.
              Then, point-level semantic annotation is performed by referring to the projection of the point cloud on the image.

              The process of obtaining the occupancy ground truth after annotation can be referred to the above figure.
              After obtaining the static superimposed point cloud annotation and dynamic target box, we first align the superimposed static background points to the ego coordinate system of the frame-by-frame point cloud, and then splice the superimposed dynamic foreground points into the point cloud according to the dynamic target posture of the frame.
              The superimposed and stitched point cloud is directly voxelized to obtain the final occupancy ground truth.
              We do not perform Poisson reconstruction on the superimposed and stitched point cloud.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="method">
    <div class="container is-max-desktop">
      <!-- 标题部分 -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Multi-Modal Fusion Network</h2>
          
          <div class="columns is-centered">
            <!-- 图片1 -->
            <div class="column">
              <div class="image-container">
                <img src="./static/images/arch_new.png" 
                    alt="Figure 1 Description"
                    class="responsive-image"
                    loading="lazy">
                <p class="image-caption">The Model Architecture</p>
              </div>
            </div>

          </div> 
          
          <!-- 描述文本 -->
          <div class="content has-text-justified">
            <p>Our occupancy perception model accepts multimodal inputs, including a LiDAR point cloud and 6 pinhole camera images.
              We use the Bird's Eye View (BEV) paradigm that has been widely validated and adopted in autonomous driving for feature extraction and feature fusion.
              Specifically, we extract LiDAR and camera features through two modality-specific feature extraction branches, and then perform multi-modal feature fusion through Transformer Decoder.
              The final occupancy result is predicted on the fused BEV features.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="results">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <!-- 标题和描述 -->
          <h2 class="title is-3">Experiments and Results</h2>
          <p>Place holder</p>
          <br>

          <!-- 视频区域 -->
          <h3 class="title is-4">Visualization Result</h3>
          <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
            <source src="https://media.githubusercontent.com/media/humanoid-occupancy/humanoid-occupancy.github.io/main/static/videos/result1.mp4" type="video/mp4">
          </video>

          <!-- 底部链接 -->
          <br>
          <br>
          <p>
            For more details on data analysis and experiment results, 
            please refer to our <a href="">paper</a>.
          </p>
        </div>
      </div>
    </div>
  </section>



  <section class="section" id="bibtex">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>place holder
      </code></pre>
    </div>
    <br>
  </section>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="">
          <i class="fas fa-file-pdf"></i>
        </a>
        <!-- <a class="icon-link" href="" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a> -->
      </div>
      <!-- <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p>
              The website template was borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
              and <a href="https://eureka-research.github.io/">Eureka</a>.
            </p>
          </div>
        </div>
      </div> -->
    </div>
  </footer>

</body>

</html>
